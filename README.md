# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**In 1-2 sentences, explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
This dataset contains customer demographic data and interactions from a bank's marketing campaign. The goal of this project is to predict whether a customer will subscribe to a term deposit, based on the given characteristics. Therefore, this problem may be summarised as a logistic regression problem, whereby the target variable has two distinct outcomes - Yes or No. 

**In 1-2 sentences, explain the solution: e.g. "The best performing model was a ..."**
The best performing model was a Automated Machine Learning (AutoML) model. This classification model outperformed the tuned Logistic Regression Model with respect to the primary metric - Accuracy.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The Scikit-Learn Pipeline used the banking data, applied feature engineering, and trained a Logistic Regression Classifier. HyperDrive was used to tune two hyperparameters: regularisation strength (--C) and iterations (max_iter).

**What are the benefits of the parameter sampler you chose?**
RandomParameterSampling was chosen due to its ability to use both continuous and discrete ranges without exhausting resources. This was particularly useful in this project due to the limited time and computing availability.

**What are the benefits of the early stopping policy you chose?**
BanditPolicy was chosen as it terminates poorly performing runs early, in line with a specified slack factor. For this project, a slack factor of 0.1 was selected, meaning that any runs that fell outside the top 10% in achieving the primary metric were terminated. Once again, this saved time and resources.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
AutoML identified the best model as an ensemble of several strong learners, following the deployment of models, inlcuding logistic regression, decision trees and boosting methods.

## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**
AutoML (~94%)achieved a greater accuracy than the best HyperDrive (~91%) model because it explored a greater number of algorithms and used automatic feature engineering. AutoML is more complex and difficult to interpret, yet its automatic nature ensured better performance. HyperDrive only tuned two hyperparameters, whereas AutoMl was optimising many more.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
For future experiments, a greater amount of time would be beneficial for identifying the optimal model. This time may be used to incorporate additional features, beyond this singular datatset, or to evaluate other metrics. In logistic regression problems, the AUC (Area under Curve) or F1 statistics are often used to compare models.

## Proof of cluster clean up
**If you did not delete your compute cluster in the code, please complete this section. Otherwise, delete this section.**
**Image of cluster marked for deletion**
